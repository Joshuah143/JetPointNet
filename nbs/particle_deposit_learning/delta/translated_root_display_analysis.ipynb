{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import awkward as ak\n",
    "import uproot as ur\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import vector\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom utility functions\n",
    "from utils.particle_data_visualize_plot_utils import *\n",
    "from utils.track_metadata import *\n",
    "from utils.data_processing_util import *\n",
    "\n",
    "def setup_environment():\n",
    "    # Append custom module paths to system path\n",
    "    sys.path.append('~/Work/LCStudies')\n",
    "    sys.path.append('~/Work/PointNet_Segmentation/')\n",
    "    \n",
    "    # Define and return base directory for all delta files\n",
    "    delta_dir = \"/data/mjovanovic/cell_particle_deposit_learning/delta/delta_processed_test_files/\"\n",
    "    return delta_dir\n",
    "\n",
    "# Usage\n",
    "delta_dir = setup_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    params = {\n",
    "        'files_name': \"delta_full\",\n",
    "        'include_delta_p_pi0': True,\n",
    "        'include_delta_n_pi0': False,\n",
    "        'include_delta_p_pipm': False,\n",
    "        'include_delta_n_pipm': True,\n",
    "        'niche_case': \"1_track_1_n_3_pi0\",  # only one niche case available 1_track_1_n_3_pi0, otherwise set to anything else ie. \"None\"\n",
    "        'len_file': 6000,\n",
    "        'i_low': 0,\n",
    "        'i_high': 4,\n",
    "        'BATCH_SIZE': 100,  # 80 for two files, 100 for other tests\n",
    "        'LOG_ENERGY_MEAN': -1,  # unrounded mean is ~ -0.93, used to normalize the log energy\n",
    "        'LOG_MEAN_TRACK_MOMETUM': 2\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Usage\n",
    "params = initialize_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PREDICTIONS \n",
    "epoch = 99\n",
    "model_file_path =  \"/data/mjovanovic/cell_particle_deposit_learning/delta_train/tr_100_val_10_tst_5_delta_1_track_1_n_3_pi0_lr_1e-2_BS_100_no_tnets_add_min_dist\"\n",
    "\n",
    "labels_unmasked = ak.Array(np.load(model_file_path + \"/tests/labels.npy\"))\n",
    "preds_unmasked = ak.Array(np.load(model_file_path + \"/tests/preds_\" + str(epoch) + \".npy\"))\n",
    "\n",
    "labels = labels_unmasked[labels_unmasked[:,:,0] != -1]\n",
    "preds = preds_unmasked[labels_unmasked[:,:,0] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "def load_dataset_and_geometry(delta_dir, params):\n",
    "    # Load dataset from preprocessed test data files\n",
    "    dataset = \"delta\"\n",
    "    file_names = [delta_dir + params['files_name'] + \"_len_\" + str(params['len_file']) + \"_i_\" + str(i) + \".npy\" for i in range(params['i_low'], params['i_high'] + 1)]\n",
    "\n",
    "    # Load cell geo tree dict\n",
    "    cell_geo_tree_file_path = \"/data/atlas/data/rho_delta/rho_small.root\"  # define the path to the ROOT file\n",
    "    file = ur.open(cell_geo_tree_file_path)\n",
    "    cell_geo_tree = file[\"CellGeo\"]\n",
    "    \n",
    "    # Extract node feature names\n",
    "    node_feature_names = cell_geo_tree.keys()[1:7]  # example to get some keys: 'cell_geo_sampling', 'cell_geo_eta', etc.\n",
    "    \n",
    "    # Load cell geo data\n",
    "    cell_geo_data = cell_geo_tree.arrays(library='np')\n",
    "    cell_geo_ID = cell_geo_data['cell_geo_ID'][0]\n",
    "    sorter = np.argsort(cell_geo_ID)\n",
    "    \n",
    "    return file_names, node_feature_names, cell_geo_data, sorter\n",
    "\n",
    "# Usage\n",
    "file_names, node_feature_names, cell_geo_data, sorter = load_dataset_and_geometry(delta_dir, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_process_event(event_data, params, event_idx, dataset):\n",
    "    num_tracks = event_data[\"nTrack\"][event_idx]\n",
    "    decay_group = event_data[\"decay_group\"][event_idx]\n",
    "\n",
    "    # Conditions for including events based on number of tracks and decay group\n",
    "    include_conditions = (\n",
    "        (dataset == \"delta\" and \n",
    "            (((num_tracks == 1) and (\n",
    "                (params['include_delta_p_pi0'] and decay_group == decay_group[\"delta+_p\"]) or\n",
    "                (params['include_delta_n_pipm'] and (decay_group == decay_group[\"delta+_n\"] or decay_group == decay_group[\"delta-\"])))) or\n",
    "            ((num_tracks == 0) and params['include_delta_n_pi0'] and decay_group == decay_group[\"delta0_n\"]) or\n",
    "            ((num_tracks == 2) and params['include_delta_p_pipm'] and (decay_group == decay_group[\"delta0_p\"] or decay_group == decay_group[\"delta++\"]))))\n",
    "        or (dataset == \"rho\" and num_tracks == 1)\n",
    "    )\n",
    "\n",
    "    return include_conditions\n",
    "\n",
    "\n",
    "def post_process_events(processed_event_data, processed_event_track_data, params, num_events_saved):\n",
    "    # Filter to BS multiple num samples/files - to make set even with preds from a trained model\n",
    "    BS_multiple_num_samples = math.floor(num_events_saved / params['BATCH_SIZE']) * params['BATCH_SIZE']\n",
    "    if BS_multiple_num_samples == num_events_saved:\n",
    "        BS_multiple_num_samples -= params['BATCH_SIZE']\n",
    "\n",
    "    for key in processed_event_data:\n",
    "        processed_event_data[key] = processed_event_data[key][:BS_multiple_num_samples]\n",
    "    for key in processed_event_track_data:\n",
    "        processed_event_track_data[key] = processed_event_track_data[key][:BS_multiple_num_samples]\n",
    "\n",
    "    return processed_event_data, processed_event_track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# cluster data dict to look up data by feature name\n",
    "processed_event_data = {}\n",
    "processed_event_track_data = {}\n",
    "processed_event_track_flags = []\n",
    "\n",
    "cell_geo_ID = cell_geo_data['cell_geo_ID'][0]\n",
    "\n",
    "samples_count = 0\n",
    "max_cells = 0\n",
    "num_cells = 0\n",
    "\n",
    "for feature in [*node_feature_names, \"cell_eta\", \"trackP\", \"trackEta\", \"trackPhi\", \"truthPartE\", \"truthPartPt\", \"truthPartEta\", \"truthPartPhi\", 'x', 'y', 'z', 'cell_hitsTruthIndex', 'cell_hitsTruthE', 'frac_pi0_energy', 'class_frac_pi0_energy', 'cell_E', 'cell_E_weight', 'sampling_layer', 'truthPartPt', 'truthPartEta', 'truthPartPhi', 'clus_idx', 'clus_em_prob', 'decay_group', 'cell_labels', 'cell_part_deposit_labels', \"E_frac_focused\", \"cell_weights\"]:#, 'delta_R']:\n",
    "    processed_event_data[feature] = []\n",
    "\n",
    "for feature in ['x', 'y', 'z', 'P', 'min_dist', 'min_eta', 'min_phi', 'sampling_layer', 'track_eta', 'track_phi', 'non_null_tracks', 'track_classes']:\n",
    "    processed_event_track_data[feature] = []\n",
    "\n",
    "cell_geo_ID = cell_geo_data['cell_geo_ID'][0]\n",
    "\n",
    "\n",
    "for file_i, preprocessed_file_name in enumerate(file_names):\n",
    "    events_arr = np.load(preprocessed_file_name, allow_pickle=True).item()\n",
    "    \n",
    "    num_events_saved = 0\n",
    "    max_cells = 0\n",
    "    num_cells = 0\n",
    "\n",
    "    num_events = len(events_arr[\"eventNumber\"])\n",
    "    event_data = events_arr\n",
    "\n",
    "    delta_n_pipm_count = 0\n",
    "    \n",
    "    print(\"LOAD FILE:\", file_i, \"/\", len(file_names))\n",
    "    for event_idx in tqdm(range(num_events)):\n",
    "        num_tracks = event_data[\"nTrack\"][event_idx]\n",
    "\n",
    "        if (len(event_data[\"cluster_cell_ID\"][event_idx]) and \\\n",
    "        (dataset == \"delta\" and \n",
    "            (((num_tracks == 1) and (\\\n",
    "                (include_delta_p_pi0 and event_data[\"decay_group\"][event_idx] == decay_group[\"delta+_p\"]) \\\n",
    "                or (include_delta_n_pipm and (event_data[\"decay_group\"][event_idx] == decay_group[\"delta+_n\"] or event_data[\"decay_group\"][event_idx] == decay_group[\"delta-\"])))) or \\\n",
    "            ((num_tracks == 0) and include_delta_n_pi0 and event_data[\"decay_group\"][event_idx] == decay_group[\"delta0_n\"]) or \\\n",
    "            ( (num_tracks == 2) and include_delta_p_pipm and (event_data[\"decay_group\"][event_idx] == decay_group[\"delta0_p\"] or event_data[\"decay_group\"][event_idx] == decay_group[\"delta++\"])))) \\\n",
    "        or (dataset == \"rho\" and num_tracks == 1)):\n",
    "\n",
    "\n",
    "            # if case where 1_track_1_n_3_pi0 need to discard 5 delta->neutron+pipm events for every 1 kept\n",
    "            if niche_case == \"1_track_1_n_3_pi0\" and (event_data[\"decay_group\"][event_idx] == decay_group[\"delta+_n\"] or event_data[\"decay_group\"][event_idx] == decay_group[\"delta-\"]):\n",
    "                if delta_n_pipm_count == 0:\n",
    "                    delta_n_pipm_count += 1\n",
    "\n",
    "                else:\n",
    "                    delta_n_pipm_count = (delta_n_pipm_count + 1) % 6\n",
    "                    continue # don't save event when delta_n_pipm_count = 1, 2, 3, 4, 5\n",
    "            \n",
    "            if num_tracks > 0:\n",
    "                x_tracks = []\n",
    "                y_tracks = []\n",
    "                z_tracks = []\n",
    "                eta_tracks = []\n",
    "                phi_tracks = []\n",
    "                rPerp_track = []\n",
    "                non_null_tracks = []\n",
    "                for track_idx in range(num_tracks):\n",
    "                    x_tracks.append([])\n",
    "                    y_tracks.append([])\n",
    "                    z_tracks.append([])\n",
    "                    eta_tracks.append([])\n",
    "                    phi_tracks.append([])\n",
    "                    rPerp_track.append([])\n",
    "                    non_null_tracks.append([])\n",
    "                    # get the eta, phi, and rPerp of each layers hit\n",
    "                    for layer_name in calo_layers:\n",
    "                        if has_fixed_r[layer_name]:\n",
    "                            eta_tracks[track_idx].append(event_data['trackEta_' + layer_name][event_idx][track_idx])\n",
    "                            phi_tracks[track_idx].append(event_data['trackPhi_' + layer_name][event_idx][track_idx])\n",
    "                            rPerp_track[track_idx].append(fixed_r[layer_name])\n",
    "                        else:\n",
    "                            eta = event_data['trackEta_' + layer_name][event_idx][track_idx]\n",
    "                            eta_tracks[track_idx].append(eta)\n",
    "                            phi_tracks[track_idx].append(event_data['trackPhi_' + layer_name][event_idx][track_idx])\n",
    "                            \n",
    "                            z = fixed_z[layer_name]\n",
    "                            aeta = np.abs(eta)\n",
    "                            rPerp = z*2*np.exp(aeta)/(np.exp(2*aeta) - 1)\n",
    "                            rPerp_track[track_idx].append(rPerp)\n",
    "                    \n",
    "                    # convert each hit to the cartesian coords\n",
    "                    thetas = [2*np.arctan(np.exp(-eta)) for eta in eta_tracks[track_idx]]\n",
    "                    x_tracks_i, y_tracks_i, z_tracks_i = spherical_to_cartesian(rPerp_track[track_idx], phi_tracks[track_idx], thetas)\n",
    "                    x_tracks[track_idx] = x_tracks_i\n",
    "                    y_tracks[track_idx] = y_tracks_i\n",
    "                    z_tracks[track_idx] = z_tracks_i\n",
    "\n",
    "                    non_null_tracks_i = np.full(NUM_TRACK_POINTS, True)\n",
    "\n",
    "                    # if eta > 4.9 or phi > pi mark the track as null (why??)\n",
    "                    non_null_tracks_i[(np.abs(eta_tracks[track_idx]) >= 2.5) | (np.abs(phi_tracks[track_idx]) > np.pi)] = False\n",
    "\n",
    "                    non_null_tracks[track_idx] = non_null_tracks_i\n",
    "\n",
    "                if dataset == \"delta\":\n",
    "                    if len(non_null_tracks) != 2:\n",
    "                        non_null_tracks = np.concatenate((non_null_tracks, np.full((1, NUM_TRACK_POINTS), False)))\n",
    "                        x_tracks = np.concatenate((x_tracks, np.full((1, NUM_TRACK_POINTS), False)))\n",
    "                        y_tracks = np.concatenate((y_tracks, np.full((1, NUM_TRACK_POINTS), False)))\n",
    "                        z_tracks = np.concatenate((z_tracks, np.full((1, NUM_TRACK_POINTS), False)))\n",
    "            else:\n",
    "                non_null_tracks = np.full((max_num_tracks, NUM_TRACK_POINTS), False)\n",
    "                x_tracks, y_tracks, z_tracks = np.zeros((max_num_tracks, NUM_TRACK_POINTS)), np.zeros((max_num_tracks, NUM_TRACK_POINTS)), np.zeros((max_num_tracks, NUM_TRACK_POINTS))\n",
    "            \n",
    "\n",
    "            cell_IDs = event_data['cluster_cell_ID'][event_idx]\n",
    "            cell_IDs = cell_IDs\n",
    "            cell_ID_map = sorter[np.searchsorted(cell_geo_ID, cell_IDs, sorter=sorter)]\n",
    "\n",
    "            # get cluster cell energy\n",
    "            cell_E = event_data[\"cluster_cell_E\"][event_idx]\n",
    "        \n",
    "            cell_weights = cell_E / np.sum(cell_E)\n",
    "\n",
    "            # node features\n",
    "            node_features = {}\n",
    "            for feature in node_feature_names:\n",
    "                node_features[feature] = cell_geo_data[feature][0][cell_ID_map]\n",
    "\n",
    "            # get cartesian coords\n",
    "            thetas = [2*np.arctan(np.exp(-eta)) for eta in node_features[\"cell_geo_eta\"]]\n",
    "            x, y, z = spherical_to_cartesian(node_features[\"cell_geo_rPerp\"], node_features[\"cell_geo_phi\"], thetas)\n",
    "\n",
    "            # label cells (4 classes)\n",
    "            # all decay groups only have 2 types particles depositing energy, a proton/neutron and a pion\n",
    "            if dataset == \"delta\":\n",
    "                class_part_idx_1 = 0\n",
    "                class_part_idx_not_1 = 0\n",
    "\n",
    "                if event_data[\"decay_group\"][event_idx] == decay_group[\"delta+_p\"]:\n",
    "                    class_part_idx_1 = part_deposit_type_class[\"track_of_interest\"] # proton\n",
    "                    class_part_idx_not_1 = part_deposit_type_class[\"pi0\"] # pi0\n",
    "                elif event_data[\"decay_group\"][event_idx] == decay_group[\"delta+_n\"] or event_data[\"decay_group\"][event_idx] == decay_group[\"delta-\"]:\n",
    "                    class_part_idx_1 = part_deposit_type_class[\"pi0\"] # neutron # TODO: update this is temp\n",
    "                    class_part_idx_not_1 = part_deposit_type_class[\"track_of_interest\"] # pi+/-\n",
    "                elif event_data[\"decay_group\"][event_idx] == decay_group[\"delta0_n\"]:\n",
    "                    class_part_idx_1 = part_deposit_type_class[\"other_neutral\"] # neutron\n",
    "                    class_part_idx_not_1 = part_deposit_type_class[\"pi0\"] # pi0\n",
    "                # if decay is delta++ or delta0_p then set labels elsewhere (since 2 tracks)\n",
    "\n",
    "                # get all cells with particle idx 1 (proton/neutron)\n",
    "                cut_part_idx_1_deposits = (ak.Array(event_data[\"cluster_cell_hitsTruthIndex\"][event_idx]) == 1)\n",
    "                # get fraction of energy from the proton/neutron (always p/n in a rho event - it deposits some energy and the pion deposits the remaining)\n",
    "                frac_cell_energy_from_part_idx_1 = ak.sum(ak.Array(event_data[\"cluster_cell_hitsTruthE\"][event_idx])[cut_part_idx_1_deposits], axis=1) / ak.sum(event_data[\"cluster_cell_hitsTruthE\"][event_idx], axis=1)\n",
    "                # if frac_cell_energy_from_part_idx_1 < 0.5 set label class_part_idx_not_1 else set cell label to class_part_idx_1\n",
    "                cell_part_deposit_labels = [class_part_idx_not_1 if cell_frac_cell_energy_from_part_idx_1 < 0.5 else class_part_idx_1 for cell_frac_cell_energy_from_part_idx_1 in frac_cell_energy_from_part_idx_1]\n",
    "            # only pi0 and pi+/- depositing energy -> binary classification\n",
    "            elif dataset == \"rho\":\n",
    "                # if the pi0 deposits the majority of the energy label cell 1 else if pi+/- deposits majority label cell 0\n",
    "                frac_pi0_energy = ak.sum(event_data[\"cluster_cell_hitsTruthE\"][event_idx][event_data[\"cluster_cell_hitsTruthIndex\"][event_idx] != 1], axis=1)/ak.sum(event_data[\"cluster_cell_hitsTruthE\"][event_idx], axis=1)\n",
    "                cell_part_deposit_labels = [1 if cell_frac_pi0_energy > 0.5 else 0 for cell_frac_pi0_energy in frac_pi0_energy]\n",
    "\n",
    "\n",
    "            # if the particle has 2 tracks match the track to the particle closest & threshold that they must be close enough together\n",
    "            # for delta dataset either delta++ -> proton + pi+/- or delta0 -> proton + pi+/-\n",
    "            track_part_dist_thresh = 1\n",
    "            if num_tracks == 2:\n",
    "\n",
    "                part1_idx = 1 # proton\n",
    "                part2_idx = 2 # charged pion\n",
    "\n",
    "                part1_phi = event_data[\"truthPartPhi\"][event_idx][part1_idx]\n",
    "                part1_eta = event_data[\"truthPartEta\"][event_idx][part1_idx]\n",
    "                part1_pt = event_data[\"truthPartPt\"][event_idx][part1_idx]\n",
    "\n",
    "                part2_phi = event_data[\"truthPartPhi\"][event_idx][part2_idx]\n",
    "                part2_eta = event_data[\"truthPartEta\"][event_idx][part2_idx]\n",
    "                part2_pt = event_data[\"truthPartPt\"][event_idx][part2_idx]\n",
    "\n",
    "                track1_phi = event_data[\"trackPhi\"][event_idx][0]\n",
    "                track1_eta = event_data[\"trackEta\"][event_idx][0]\n",
    "                track1_pt = event_data[\"trackPt\"][event_idx][0]\n",
    "\n",
    "                track2_phi = event_data[\"trackPhi\"][event_idx][1]\n",
    "                track2_eta = event_data[\"trackEta\"][event_idx][1]\n",
    "                track2_pt = event_data[\"trackPt\"][event_idx][1]\n",
    "\n",
    "                part1_track1_dist = measure_track_part_dists(track1_phi, track1_eta, track1_pt, part1_phi, part1_eta, part1_pt)\n",
    "                part1_track2_dist = measure_track_part_dists(track2_phi, track2_eta, track2_pt, part1_phi, part1_eta, part1_pt)\n",
    "                part2_track1_dist = measure_track_part_dists(track1_phi, track1_eta, track1_pt, part2_phi, part2_eta, part2_pt)\n",
    "                part2_track2_dist = measure_track_part_dists(track2_phi, track2_eta, track2_pt, part2_phi, part2_eta, part2_pt)\n",
    "\n",
    "                # either pair part1 with track1 and part2 with track2 or part1 with track2 and part2 with track1\n",
    "                # or discard event if no pairing exists with both track-part dists < thresh\n",
    "                paring_one_sum_dist = part1_track1_dist + part2_track2_dist if part1_track1_dist < track_part_dist_thresh and part2_track2_dist < track_part_dist_thresh else 2*track_part_dist_thresh\n",
    "                paring_two_sum_dist = part1_track2_dist + part2_track1_dist if part1_track1_dist < track_part_dist_thresh and part2_track2_dist < track_part_dist_thresh else 2*track_part_dist_thresh\n",
    "                \n",
    "                if max(paring_one_sum_dist, paring_two_sum_dist) >= 2*track_part_dist_thresh:\n",
    "                    num_tracks = 0\n",
    "                else:\n",
    "                    if paring_one_sum_dist < paring_two_sum_dist:\n",
    "                        pairing_one = True\n",
    "                    else:\n",
    "                        pairing_one = False\n",
    "\n",
    "            track_idx = 0\n",
    "            added_one_sample = False # for each event add one sample to dataset\n",
    "\n",
    "            non_null_tracks = np.array(non_null_tracks)\n",
    "            x_tracks = np.array(x_tracks)\n",
    "            y_tracks = np.array(y_tracks)\n",
    "            z_tracks = np.array(z_tracks)\n",
    "            #non_null_tracks = np.array(flatten_one_layer(non_null_tracks))\n",
    "            #x_tracks = np.array(flatten_one_layer(x_tracks))\n",
    "            #y_tracks = np.array(flatten_one_layer(y_tracks))\n",
    "            #z_tracks = np.array(flatten_one_layer(z_tracks))\n",
    "            \n",
    "            x_tracks[~non_null_tracks] = 0\n",
    "            y_tracks[~non_null_tracks] = 0\n",
    "            z_tracks[~non_null_tracks] = 0\n",
    "\n",
    "            cell_has_E_deposit = ak.sum(event_data[\"cluster_cell_hitsTruthE\"][event_idx], axis=1) > 0\n",
    "            num_cells = len(cell_E[cell_has_E_deposit])\n",
    "            \n",
    "            # execute once for 0-1 track, and 2 times for 2 tracks\n",
    "            while not added_one_sample or track_idx < num_tracks:\n",
    "                processed_event_data[\"cell_E\"].append(cell_E[cell_has_E_deposit])\n",
    "                processed_event_data[\"x\"].append(x[cell_has_E_deposit])\n",
    "                processed_event_data[\"y\"].append(y[cell_has_E_deposit])\n",
    "                processed_event_data[\"z\"].append(z[cell_has_E_deposit])\n",
    "                processed_event_data[\"cell_weights\"].append(cell_weights[cell_has_E_deposit])  \n",
    "\n",
    "                # extra features for analysis\n",
    "                processed_event_data[\"clus_idx\"].append(ak.Array(event_data[\"clus_idx\"][event_idx][cell_has_E_deposit]))\n",
    "                processed_event_data[\"clus_em_prob\"].append(ak.Array(event_data[\"clus_em_prob\"][event_idx][cell_has_E_deposit]))\n",
    "                processed_event_data[\"sampling_layer\"].append(ak.Array(node_features[\"cell_geo_sampling\"][cell_has_E_deposit]))\n",
    "                processed_event_data[\"cell_hitsTruthIndex\"].append(ak.Array(event_data[\"cluster_cell_hitsTruthIndex\"][event_idx][cell_has_E_deposit]))\n",
    "                processed_event_data[\"cell_hitsTruthE\"].append(ak.Array(event_data[\"cluster_cell_hitsTruthE\"][event_idx][cell_has_E_deposit]))\n",
    "                processed_event_data[\"cell_eta\"].append(node_features[\"cell_geo_eta\"])\n",
    "\n",
    "                processed_event_data[\"truthPartPt\"].append(ak.Array(event_data[\"truthPartPt\"][event_idx]))\n",
    "                processed_event_data[\"truthPartE\"].append(ak.Array(event_data[\"truthPartE\"][event_idx]))\n",
    "                processed_event_data[\"truthPartEta\"].append(ak.Array(event_data[\"truthPartEta\"][event_idx]))\n",
    "                processed_event_data[\"truthPartPhi\"].append(ak.Array(event_data[\"truthPartPhi\"][event_idx]))  \n",
    "\n",
    "                processed_event_data[\"trackPhi\"].append(ak.Array(event_data[\"trackPhi\"][event_idx]))  \n",
    "                processed_event_data[\"trackEta\"].append(ak.Array(event_data[\"trackEta\"][event_idx]))  \n",
    "                processed_event_data[\"trackP\"].append(ak.Array(event_data[\"trackP\"][event_idx]))  \n",
    "\n",
    "                processed_event_data[\"decay_group\"].append(event_data[\"decay_group\"][event_idx])\n",
    "                processed_event_data[\"E_frac_focused\"].append(np.array(frac_cell_energy_from_part_idx_1)[cell_has_E_deposit])\n",
    "                processed_event_data[\"cell_part_deposit_labels\"].append(np.array(cell_part_deposit_labels)[cell_has_E_deposit])\n",
    "\n",
    "                \n",
    "\n",
    "                if dataset == \"delta\":\n",
    "                    track_classes = np.zeros((2, NUM_TRACK_POINTS))\n",
    "                    track_Ps = np.zeros((2, NUM_TRACK_POINTS))\n",
    "                else:\n",
    "                    track_classes = np.zeros((1, NUM_TRACK_POINTS))\n",
    "                    track_Ps = np.zeros((1, NUM_TRACK_POINTS))\n",
    "                        \n",
    "                if num_tracks == 2:\n",
    "                    # set track_idx to be the track of interest (for track idx 0 track 0 is of interest, else track 1)\n",
    "                    if track_idx == 0:\n",
    "                        track_classes[0] = np.ones(NUM_TRACK_POINTS)\n",
    "                        track_classes[1] = np.full(NUM_TRACK_POINTS, 2)\n",
    "                    else:\n",
    "                        track_classes[1] = np.ones(NUM_TRACK_POINTS)\n",
    "                        track_classes[0] = np.full(NUM_TRACK_POINTS, 2)\n",
    "\n",
    "                    if (pairing_one and track_idx == 0) or (not pairing_one and track_idx == 1): # pair track 0 and part 1\n",
    "                        class_part_idx_1 = 0 # track of interest\n",
    "                        class_part_idx_not_1 = 1 # other tracked charged particle \n",
    "\n",
    "                    else: # pairing 1 and track idx == 1 or paring 2 and track idx == 0\n",
    "                        class_part_idx_1 = 1\n",
    "                        class_part_idx_not_1 = 0\n",
    "\n",
    "                    cell_part_deposit_labels = [class_part_idx_not_1 if cell_frac_cell_energy_from_part_idx_1 < 0.5 else class_part_idx_1 for cell_frac_cell_energy_from_part_idx_1 in frac_cell_energy_from_part_idx_1]\n",
    "                    #print(\"cell_labels:\", cell_labels)\n",
    "                    track_1_P =  np.log10((event_data[\"trackP\"][event_idx][0])) - LOG_ENERGY_MEAN\n",
    "                    track_2_P =  np.log10((event_data[\"trackP\"][event_idx][1])) - LOG_ENERGY_MEAN\n",
    "                    track_Ps[0] = np.full(NUM_TRACK_POINTS, track_1_P)\n",
    "                    track_Ps[1] = np.full(NUM_TRACK_POINTS, track_2_P)\n",
    "                \n",
    "                elif num_tracks == 1:\n",
    "                    # get tracks momentum readout\n",
    "                    track_P = np.log10(event_data['trackP'][event_idx][0]) - LOG_MEAN_TRACK_MOMETUM\n",
    "                    track_Ps[0] = np.full(NUM_TRACK_POINTS, track_P)\n",
    "                    track_classes[0] = np.ones(NUM_TRACK_POINTS)\n",
    "\n",
    "                #track_classes = np.array(flatten_one_layer(track_classes))\n",
    "                #track_Ps = np.array(flatten_one_layer(track_Ps))\n",
    "\n",
    "                track_classes[~non_null_tracks] = 0\n",
    "                track_Ps[~non_null_tracks] = 0\n",
    "                \n",
    "                # else no tracks => Pt = 0\n",
    "\n",
    "\n",
    "                processed_event_track_data[\"x\"].append(x_tracks)\n",
    "                processed_event_track_data[\"y\"].append(y_tracks)\n",
    "                processed_event_track_data[\"z\"].append(z_tracks)\n",
    "                processed_event_track_data[\"P\"].append(track_Ps) # don't normalize for analysis\n",
    "\n",
    "                # track classes - 0 => point, 1 => track of interest, 2 => other track\n",
    "                processed_event_track_data[\"track_classes\"].append(track_classes)\n",
    "                \n",
    "                if num_cells + NUM_TRACK_POINTS*max_num_tracks > max_cells:\n",
    "                    max_cells = num_cells + NUM_TRACK_POINTS*max_num_tracks\n",
    "\n",
    "                num_events_saved += 1\n",
    "                added_one_sample = True\n",
    "                track_idx += 1\n",
    "\n",
    "    if BATCH_SIZE:\n",
    "        # filter to BS multiple num samples/files - to make set even with preds from a trained model\n",
    "        BS_multiple_num_samples = math.floor(num_events_saved / BATCH_SIZE)*BATCH_SIZE\n",
    "        print(\"BS_multiple_num_samples\", BS_multiple_num_samples)\n",
    "        print(\"num_events_saved\", num_events_saved)\n",
    "        if BS_multiple_num_samples == num_events_saved:\n",
    "            BS_multiple_num_samples -= BATCH_SIZE\n",
    "        for key in processed_event_data:\n",
    "            del processed_event_data[key][BS_multiple_num_samples - num_events_saved:]\n",
    "\n",
    "        for key in processed_event_track_data:\n",
    "            del processed_event_track_data[key][BS_multiple_num_samples - num_events_saved:]\n",
    "\n",
    "        print(\"num dropped events:\", num_events_saved - BS_multiple_num_samples)\n",
    "        num_events_saved = BS_multiple_num_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
