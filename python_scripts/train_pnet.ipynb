{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "import math\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append('/home/jbohm/start_tf/PointNet_Segmentation')\n",
    "from pnet_models_updated import pnet_part_seg_no_tnets, pnet_part_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6\"\n",
    "\n",
    "# disable eager execution with tensorflow (since can't execute lambda functions with eager execution)\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AND OUTPUT DIRS\n",
    "data_dir = '/fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_1' # parent directory that holds train, val, and test files\n",
    "train_data_dir = data_dir + '/train/'\n",
    "val_data_dir = data_dir + '/val/'\n",
    "test_data_dir = data_dir + '/test/'\n",
    "\n",
    "output_dir = \"/fast_scratch_1/jbohm/cell_particle_deposit_learning/train_dirs/pnet_train_1/tr_5_val_1_tst_1_lr_1e-2_BS_100_no_tnets\" # save model and predictions to this dir\n",
    "max_points_file = '/max_points.txt' # load the max number of points in a sample\n",
    "\n",
    "num_train_files = 5\n",
    "num_val_files = 1\n",
    "num_test_files = 1\n",
    "events_per_file = 6000 # approx number of samples in a file\n",
    "start_at_epoch = 0 # to load a pre trained model from this epoch\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATORS\n",
    "def batched_data_generator(file_names, batch_size, max_num_points, loop_infinite=True):\n",
    "    while True:\n",
    "        for file in file_names:\n",
    "            point_net_data = np.load(file)\n",
    "            event_data = point_net_data['X']\n",
    "            Y = point_net_data['Y']\n",
    "\n",
    "            # pad X and Y data to have y dimension of max_num_points\n",
    "            X_padded = np.zeros((event_data.shape[0], max_num_points, event_data.shape[2])) # pad with zeros\n",
    "            Y_padded = np.negative(np.ones(((event_data.shape[0], max_num_points, 1)))) # pad with -1s\n",
    "            \n",
    "            for i, event in enumerate(event_data):                \n",
    "                X_padded[i, :len(event), :5] = event\n",
    "                Y_padded[i, :len(event), :] = Y[i]\n",
    "    \n",
    "            # split into batch_size groups of events\n",
    "            for i in range(1, math.ceil(event_data.shape[0]/batch_size)):\n",
    "                yield X_padded[(i-1)*batch_size:i*batch_size], Y_padded[(i-1)*batch_size:i*batch_size]\n",
    "\n",
    "        if not loop_infinite:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS\n",
    "def masked_bce_pointwise_loss(y_true, y_pred):\n",
    "    y_true = tf.expand_dims(y_true[:,:,0], -1)\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "    return K.sum(K.binary_crossentropy(tf.multiply(y_pred, mask), tf.multiply(y_true, mask)), axis=None) / K.sum(mask, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP TRAIN, VAL, AND TEST GENERATORS\n",
    "train_files = np.sort(glob.glob(train_data_dir+'*.npz'))[:num_train_files]\n",
    "val_files = np.sort(glob.glob(val_data_dir+'*.npz'))[:num_val_files]\n",
    "test_files = np.sort(glob.glob(test_data_dir+'*.npz'))[:num_test_files]\n",
    "\n",
    "num_batches_train = (len(train_files) * events_per_file) / BATCH_SIZE \n",
    "num_batches_val = (len(val_files) * events_per_file) / BATCH_SIZE\n",
    "num_batches_test = (len(test_files) * events_per_file) / BATCH_SIZE\n",
    "\n",
    "# load the max number of points (N) - saved to data dir\n",
    "with open(data_dir + max_points_file) as f:\n",
    "    N = int(f.readline())\n",
    "\n",
    "train_generator = batched_data_generator(train_files, BATCH_SIZE, N)\n",
    "val_generator = batched_data_generator(val_files, BATCH_SIZE, N)\n",
    "test_generator = batched_data_generator(test_files, BATCH_SIZE, N, loop_infinite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 09:37:14.607479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9159 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b1:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "features_64_tdist (TimeDistribu (None, None, 64)     320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_64 (MaskedBa (None, None, 64)     128         features_64_tdist[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "features_64_relu (Activation)   (None, None, 64)     0           batchNorm_features_64[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "features_128_1_tdist (TimeDistr (None, None, 128)    8320        features_64_relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_128_1 (Maske (None, None, 128)    256         features_128_1_tdist[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "features_128_1_relu (Activation (None, None, 128)    0           batchNorm_features_128_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "features_128_2_tdist (TimeDistr (None, None, 128)    16512       features_128_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_128_2 (Maske (None, None, 128)    256         features_128_2_tdist[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "features_128_2_relu (Activation (None, None, 128)    0           batchNorm_features_128_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "features_512_tdist (TimeDistrib (None, None, 512)    66048       features_128_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_features_512 (MaskedB (None, None, 512)    1024        features_512_tdist[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "features_512_relu (Activation)  (None, None, 512)    0           batchNorm_features_512[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pre_maxpool_block_tdist (TimeDi (None, None, 2048)   1050624     features_512_relu[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_pre_maxpool_block (Ma (None, None, 2048)   4096        pre_maxpool_block_tdist[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pre_maxpool_block_relu (Activat (None, None, 2048)   0           batchNorm_pre_maxpool_block[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "pre_maxpool_block_masked (Lambd (None, None, 2048)   0           pre_maxpool_block_relu[0][0]     \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_features (MaxPooling1D)  (None, None, 2048)   0           pre_maxpool_block_masked[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile (TensorFlowOpL [(None, None, 2048)] 0           global_features[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "segmentation_input (Concatenate (None, None, 2880)   0           features_64_relu[0][0]           \n",
      "                                                                 features_128_1_relu[0][0]        \n",
      "                                                                 features_128_2_relu[0][0]        \n",
      "                                                                 features_512_relu[0][0]          \n",
      "                                                                 tf_op_layer_Tile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "segmentation_features_tdist (Ti (None, None, 128)    368768      segmentation_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batchNorm_segmentation_features (None, None, 128)    256         segmentation_features_tdist[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "segmentation_features_relu (Act (None, None, 128)    0           batchNorm_segmentation_features[0\n",
      "__________________________________________________________________________________________________\n",
      "last_tdist (TimeDistributed)    (None, None, 1)      129         segmentation_features_relu[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "last_act (Activation)           (None, None, 1)      0           last_tdist[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,516,737\n",
      "Trainable params: 1,516,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# COMPILE MODEL\n",
    "model = pnet_part_seg_no_tnets(N)\n",
    "model.compile(loss=masked_bce_pointwise_loss, optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS\n",
    "# make directories if not present\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(output_dir + \"/weights\"):\n",
    "    os.makedirs(output_dir + \"/weights\")\n",
    "if not os.path.exists(output_dir + \"/tests\"):\n",
    "    os.makedirs(output_dir + \"/tests\")\n",
    "\n",
    "# save preds, model weights, and train/val loss after each epoch\n",
    "class SaveEpoch(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # save preds on new test file\n",
    "        per_epoch_test_generator = batched_data_generator(test_files, BATCH_SIZE, N, loop_infinite=False)\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        for X_test, Y_test in per_epoch_test_generator:\n",
    "            predictions.extend(model.predict(X_test))\n",
    "            labels.extend(Y_test)\n",
    "        np.save(output_dir + \"/tests/preds_\" + str(start_at_epoch + epoch) + \".npy\", predictions)\n",
    "        if epoch == 0:\n",
    "            # save the labels for up to 5 test files\n",
    "            np.save(output_dir + \"/tests/labels.npy\", labels)\n",
    "\n",
    "        # save model weights\n",
    "        model.save_weights(output_dir + \"/weights/weights_\" + str(start_at_epoch + epoch) + \".h5\")\n",
    "        # save loss\n",
    "        with open(output_dir + \"/log_loss.csv\" ,'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([start_at_epoch + epoch , logs[\"loss\"], logs[\"val_loss\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 09:37:42.799317: I tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA: 0s - batch: 149.5000 - size: 100.0000 - loss: 5.4781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'add_min_track_dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_generator,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_generator,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mnum_batches_train,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mnum_batches_val,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[SaveEpoch()])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    776\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 777\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    778\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    779\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    780\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    781\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    782\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    783\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    784\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    785\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    786\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    787\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    788\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    789\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    790\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    791\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    792\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    793\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    794\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    795\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    796\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:570\u001b[0m, in \u001b[0;36mGeneratorOrSequenceTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    567\u001b[0m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps_per_epoch, x)\n\u001b[1;32m    568\u001b[0m training_utils_v1\u001b[39m.\u001b[39mcheck_generator_arguments(\n\u001b[1;32m    569\u001b[0m     y, sample_weight, validation_split\u001b[39m=\u001b[39mvalidation_split)\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m fit_generator(\n\u001b[1;32m    571\u001b[0m     model,\n\u001b[1;32m    572\u001b[0m     x,\n\u001b[1;32m    573\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    574\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    575\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    576\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    577\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    578\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    579\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    580\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    581\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    582\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    583\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    584\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    585\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    586\u001b[0m     steps_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msteps_per_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_generator_v1.py:315\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m   epoch_logs \u001b[39m=\u001b[39m cbks\u001b[39m.\u001b[39mmake_logs(\n\u001b[1;32m    311\u001b[0m       model, epoch_logs, val_results, mode, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m ModeKeys\u001b[39m.\u001b[39mTRAIN:\n\u001b[1;32m    314\u001b[0m   \u001b[39m# Epochs only apply to `fit`.\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m   callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m    317\u001b[0m \u001b[39m# Recreate dataset iterator for the next epoch.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m reset_dataset_after_each_epoch \u001b[39mand\u001b[39;00m epoch \u001b[39m<\u001b[39m epochs \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:413\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    412\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 413\u001b[0m   callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "\u001b[1;32m/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb Cell 9\u001b[0m in \u001b[0;36mSaveEpoch.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m{}):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# save preds on new test file\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     per_epoch_test_generator \u001b[39m=\u001b[39m batched_data_generator(test_files, BATCH_SIZE, N, loop_infinite\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, add_min_track_dist\u001b[39m=\u001b[39madd_min_track_dist)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     predictions \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btriumf-ml1.phas.ubc.ca/home/jbohm/start_tf/PointNet_Segmentation/python_scripts/train_pnet.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     labels \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'add_min_track_dist' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1,\n",
    "    steps_per_epoch=num_batches_train,\n",
    "    validation_steps=num_batches_val,\n",
    "    callbacks=[SaveEpoch()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
